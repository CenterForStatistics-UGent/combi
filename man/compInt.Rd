% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/F_compInt.R
\name{compInt}
\alias{compInt}
\title{Perform model-based data integration}
\usage{
compInt(data, M = 3L, covariates = NULL, distributions, compositional,
  maxIt = 300L, tol = 0.001, verbose = FALSE, prevCutOff = 0.95,
  minFraction = 0.1, logTransformMicroArray = TRUE,
  confounders = NULL, nleq.control = list(maxit = 1000L, cndtol =
  1e-16), record = TRUE, weights = NULL, fTol = 1e-05, nCores = 1,
  meanVarFit = "spline", maxFeats = 2000, dispFreq = 10L,
  allowMissingness = FALSE, biasReduction = TRUE, maxItFeat = 20L,
  maxItFilt = 50L)
}
\arguments{
\item{data}{a list of data matrices with the same number of samples n in the rows.
Also phyloseq objects are acceptable}

\item{M}{the required dimension of the fit, a non-negative integer}

\item{covariates}{a dataframe of n samples with sample-specific variables.}

\item{distributions}{a character vector describing which distributional assumption should be used. See details.}

\item{compositional}{A logical vector with the same length as "data",
indicating if the datasets should be treated as compositional}

\item{maxIt}{an integer, the maximum number of iterations}

\item{tol}{A small scalar, the convergence tolerance}

\item{verbose}{Logical. Should verbose output be printed to the console?}

\item{prevCutOff}{a scalar, the prevalance cutoff for the trimming.}

\item{minFraction}{a scalar, each taxon's total abundance
should equal at least the number of samples n times minFraction,
  otherwise it is trimmed.}

\item{logTransformMicroArray}{A boolean, should the array data be logtransformed?}

\item{confounders}{A dataframe or a list of dataframes with the same length as data.
In the former case the same dataframe is used for conditioning,
In the latter case each view has its own conditioning variables (or NULL).}

\item{nleq.control}{A list of arguments to the nleqslv function}

\item{record}{A boolean, should intermediate estimates be stored? Can be useful to check convergence}

\item{weights}{A character string, either 'marginal' or 'uniform', indicating
rrhow the feature parameters should be weighted in the normalization}

\item{fTol}{The tolerance for solving the estimating equations}

\item{nCores}{The number of cores to be used in estimating the feature parameters of each view. See details.}

\item{meanVarFit}{The type of mean variance fit, see details}

\item{maxFeats}{The maximal number of features for a Newton-Raphson procedure
to be feasible}

\item{dispFreq}{An integer, the period after which the variances should be
reestimated}

\item{allowMissingness}{A boolean, should NA values be allowed?}

\item{biasReduction}{A boolean, should bias reduction be applied to allow for
confounder correction in groups with all zeroes? Not guaranteed to work}

\item{maxItFeat, maxItFilt}{Integers, the maximum allowed number of iterations
in the estimation of the feature parametes and confounder parameters
respectively}
}
\value{
An object of the "compInt" class, containing all information on the
data integration and fitting procedure

An object of class compInt, describing the fit
}
\description{
Perform model-based data integration
}
\details{
Using more than one core is only implemented on Unix systems.
Setting nCores > 1 on Windows will use a single core, with a warning.
When the number of cores specified is larger than the number of views,
nCores is silently set to the number of views.
meanVarFit = "spline" yields a cubic spline fit for the abundance-variance
 trend, "cubic" gives a third degree polynomial. Both converge to the
 diagonal line with slope 1 for small means.
}
\examples{
data(hmp2)
microVirDI = compInt(data = list("microbiome" = microPruneVir,
"virome" = virPrune), distributions = c("quasi", "quasi"),
compositional = c(TRUE, TRUE), verbose = TRUE, nCores = 1, M = 2)
}
